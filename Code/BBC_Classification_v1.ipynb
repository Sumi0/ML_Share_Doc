{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        \n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_dir = '/kaggle/input/learn-ai-bbc/'\ntrain_file = file_dir + 'BBC News Train.csv'\ntest_file = file_dir + 'BBC News Test.csv'\n\nsubmit_file = file_dir + 'BBC News Sample Solution.csv'\nglove50d_file_path = '/kaggle/input/gloveweights/glove.6B.50d.txt'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"trainData = pd .read_csv(train_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainData.Category.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def changeLabel(s):\n    if s == 'sport':\n        return 0;\n    if s == 'business':\n        return 1;\n    if s == 'politics':\n        return 2;\n    if s == 'entertainment':\n        return 3;\n    if s == 'tech':\n        return 4;\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(changeLabel('business'))\ntrainData['Category'] = trainData['Category'].apply(lambda x : changeLabel(x) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainData['Category'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = trainData['Text'].values\n\ny = tf.keras.utils.to_categorical(trainData['Category'].values)\n\nsentences_train, sentences_test, y_train, y_test = train_test_split(\n    sentences, y, test_size=0.20, random_state=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=5000)\n\ntokenizer.fit_on_texts(sentences_train)\n\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_test = tokenizer.texts_to_sequences(sentences_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train[:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1 \nprint(vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = 'ab cd efg jk'\nprint(len(s.split(' ')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = trainData[['Text']]\nlenList = []\n#text.apply(lambda x : print(len(x) )) \nfor s in text.Text.values :\n    lenList.append(len(s.split(' ')))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(lenList)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# length "},{"metadata":{"trusted":true},"cell_type":"code","source":"lenList.sort()\nlenList[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lenList.sort(reverse=True)\nlenList[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 3000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras.models import Sequential\n#from keras import layers\nimport tensorflow as tf\n\nm = tf.keras.models\n\nlayers = tf.keras.layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocab size is 1750 \n# input_length is size of review text after tokenization and pad sequance\nembedding_dim = 120\nmodel = m.Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size,\n                           output_dim=embedding_dim,\n                           input_length=maxlen))\n\n# model.add(layers.Flatten())\n#model.add(layers.GlobalMaxPool1D())\n\nmodel.add(layers.LSTM(200, activation='relu')) \n#model.add(layers.Dense(1024, activation='relu')) # 1024 -- 94.3\n#model.add(layers.Dropout(0.2)) # -ve impactt\n#model.add(layers.Dense(512, activation='relu'))\n\nmodel.add(layers.Dense(50, activation='relu'))\n#model.add(layers.Dropout(0.2)) # -ve impact\nmodel.add(layers.Dense(5, activation='softmax'))\n\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer= tf.keras.optimizers.Adam(lr=.0005),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                    epochs=20,verbose=True,\n                    validation_data=(X_test, y_test),\n                    batch_size = 10)\n# batch size 10 or 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding dim\nmaxlen\nLearning rate of optimizer\n.001\n\n\nadam = \nmodel.compile(optimizer = tf.keras.optimizers.Adam(lr=.0005),\n                   loss = 'categorical_crossentropy',\n                metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_embedding_matrix(filepath, word_index, embedding_dim):\n    \n    vocab_size = len(word_index) + 1 \n    # Adding again 1 because of reserved 0 index\n    \n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n    with open(filepath) as file:\n        \n        for line in file:\n            \n            # Fetching word and weights of that word from pre-trained file\n            word, *vector = line.split()\n            \n            # putting word only if it exist in text for which model is created\n            if word in word_index:\n                # Find token number of each word from Toekenzier\n                idx = word_index[word]\n                #print(\"{} {} \".format(word,idx))\n                # Create matrix with toekn and its vector from pre-trained weights\n                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 50\nembedding_matrix = create_embedding_matrix(glove50d_file_path,\n                                           tokenizer.word_index, \n                                           embedding_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocab size is 1750 \n# input_length is size of review text after tokenization and pad sequance\nembedding_dim = 50\nmodel1 = m.Sequential()\nmodel1.add(layers.Embedding(input_dim=vocab_size,\n                           output_dim=embedding_dim,\n                           weights=[embedding_matrix],\n                           trainable=True, \n                           input_length=maxlen))\n\n# model.add(layers.Flatten())\nmodel1.add(layers.GlobalMaxPool1D())\n\n\nmodel1.add(layers.Dense(1024, activation='relu')) # 1024 -- 94.3\n#model.add(layers.Dropout(0.2)) # -ve impactt\n#model.add(layers.Dense(512, activation='relu'))\n\nmodel1.add(layers.Dense(100, activation='relu'))\n#model.add(layers.Dropout(0.2)) # -ve impact\nmodel1.add(layers.Dense(5, activation='softmax'))\n\n\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.compile(optimizer= tf.keras.optimizers.Adam(lr=.0005),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model1.fit(X_train, y_train,\n                    epochs=40,verbose=True,\n                    validation_data=(X_test, y_test),\n                    batch_size = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#embedding_matrix[0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testFile = pd.read_csv(test_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testFile.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}